{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bcede47-3e42-4488-8763-ac392c712aa6",
   "metadata": {},
   "source": [
    "### Value Iteration for Stochastic MDPs\n",
    "\n",
    "**Value Iteration** is a model-based dynamic programming algorithm that directly computes the optimal value function $V^*(s)$ without explicit policy representation.\n",
    "\n",
    "**Key Principle**: Apply Bellman optimality operator iteratively:\n",
    "$$V_{k+1}(s) = \\max_a \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V_k(s')]$$\n",
    "\n",
    "**Algorithm**: Initialize $V_0$ → Iterate Bellman operator → Extract optimal policy $\\pi^*(s) = \\arg\\max_a Q^*(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a6d2b3",
   "metadata": {},
   "source": [
    "#### Environment Setup\n",
    "\n",
    "Using **stochastic FrozenLake** environment where transitions are probabilistic, demonstrating value iteration's effectiveness with uncertain dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef8f5af1-e73b-477d-944a-e036ba7818ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7d4f998-cfef-4e28-9238-7bdcdfce6dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', map_name=\"4x4\", render_mode=\"rgb_array\", is_slippery=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad5dafd",
   "metadata": {},
   "source": [
    "#### Policy Visualization Helper\n",
    "\n",
    "Utility function to display learned policy in grid format for easy interpretation of agent's action choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5184caf2-7383-4691-8ae8-a85e47043c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy(policy, grid=(4,4)):\n",
    "    action_dict = {0: \"Left\", 1: \"Down\", 2: \"Right\", 3: \"Up\"}\n",
    "    policy_print = np.empty(grid).astype(str)\n",
    "    for idx_h in range(grid[0]):\n",
    "        for idx_w in range(grid[1]):\n",
    "            index = idx_h * grid[0] + idx_w\n",
    "            selected_action = action_dict[policy[index]]\n",
    "            selected_action = selected_action[0] \n",
    "            policy_print[idx_h, idx_w] = selected_action\n",
    "\n",
    "    print(\"Policy:\")\n",
    "    print(policy_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f67bbb3",
   "metadata": {},
   "source": [
    "#### Value Iteration Algorithm\n",
    "\n",
    "**Core Implementation**: For each state, compute Q-values for all actions and take maximum. Iterate until convergence when $\\|V_{k+1} - V_k\\|_{\\infty} < \\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55cc18ec-0111-438b-9a3d-cc53a2d2857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=0.99, num_iterations=1000, tol=1e-5):\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        V_k = np.copy(V)\n",
    "\n",
    "        for s in range(env.observation_space.n):\n",
    "            Q_s = []\n",
    "            \n",
    "            # Take every action from current state and compute Q_value for every action\n",
    "            for wanted_action in range(env.action_space.n):\n",
    "                possible_actions = env.unwrapped.P[s][wanted_action]\n",
    "\n",
    "                Q_sa = 0\n",
    "                for probability, s_next, reward, terminal in possible_actions:\n",
    "                    Q_sa += probability * (reward + gamma * V_k[s_next])\n",
    "\n",
    "                Q_s.append(Q_sa)\n",
    "\n",
    "            V[s] = np.max(Q_s)\n",
    "\n",
    "        if np.max(np.abs(V - V_k)) < tol:\n",
    "            break\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76dd733",
   "metadata": {},
   "source": [
    "#### Compute Optimal Value Function\n",
    "\n",
    "Run value iteration to find $V^*(s)$ - the maximum expected cumulative reward achievable from each state under optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c128466b-9b2c-4f00-b667-9db7412d9f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Values\n",
      "[[0.54185998 0.49858161 0.47043461 0.45657012]\n",
      " [0.55829709 0.         0.35822941 0.        ]\n",
      " [0.59166815 0.64298202 0.6151213  0.        ]\n",
      " [0.         0.74165099 0.86280139 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "optimal_values = value_iteration(env)\n",
    "\n",
    "print(\"Optimal Values\")\n",
    "print(np.array(optimal_values).reshape(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebd0496",
   "metadata": {},
   "source": [
    "#### Policy Extraction\n",
    "\n",
    "**Extract optimal policy** from value function: $\\pi^*(s) = \\arg\\max_a \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V^*(s')]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4772a928-59cc-483f-a7ca-2677eb01651f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(values, gamma=0.99):\n",
    "    new_policy = np.zeros(env.observation_space.n)\n",
    "\n",
    "    for s in range(env.observation_space.n):\n",
    "        Q_s = []\n",
    "\n",
    "        for wanted_action in range(env.action_space.n):\n",
    "            possible_actions = env.unwrapped.P[s][wanted_action]\n",
    "\n",
    "            Q_sa = 0\n",
    "            for probability, s_next, reward, terminal in possible_actions:\n",
    "                Q_sa += probability * (reward + gamma * values[s_next])\n",
    "\n",
    "            Q_s.append(Q_sa)\n",
    "\n",
    "        best_action = np.argmax(Q_s) \n",
    "\n",
    "        new_policy[s] = best_action\n",
    "\n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51736438-a041-4d09-8bf2-1e52e5c98dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:\n",
      "[['L' 'U' 'U' 'U']\n",
      " ['L' 'L' 'L' 'L']\n",
      " ['U' 'D' 'L' 'L']\n",
      " ['L' 'R' 'D' 'L']]\n"
     ]
    }
   ],
   "source": [
    "optimal_policy = policy_improvement(optimal_values)\n",
    "print_policy(optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7fceef-8688-4cf3-afbf-775c39c4aff1",
   "metadata": {},
   "source": [
    "#### Performance Evaluation\n",
    "\n",
    "Test the learned optimal policy in the stochastic environment to measure success rate and validate the effectiveness of value iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe718a63-a930-4820-8f3a-6fae96d6d402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of Successful Games: 0.82\n"
     ]
    }
   ],
   "source": [
    "num_games = 1000\n",
    "max_steps = 500\n",
    "\n",
    "game_success = 0\n",
    "for _ in range(num_games):\n",
    "    observation, _ = env.reset()\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "        action = int(optimal_policy[observation])\n",
    "        \n",
    "        observation, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            if reward > 0:\n",
    "                game_success += 1\n",
    "            break\n",
    "\n",
    "proportion_sucessful = game_success / num_games\n",
    "print(\"Proportion of Successful Games:\", proportion_sucessful)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "959fe9b7-cd32-4cb7-a6fd-4a02a40136d5",
   "metadata": {},
   "source": [
    "### Policy Iteration for Stochastic Environments\n",
    "\n",
    "#### Introduction to Stochastic MDPs\n",
    "\n",
    "In **stochastic environments**, the outcome of taking an action is not deterministic. Unlike deterministic MDPs where $P(s'|s,a) \\in \\{0,1\\}$, stochastic MDPs have probabilistic transitions where:\n",
    "\n",
    "$$\\sum_{s' \\in S} P(s'|s,a) = 1 \\quad \\forall s \\in S, a \\in A$$\n",
    "\n",
    "This introduces **uncertainty** into the decision-making process, making optimal policy computation more challenging but also more realistic for many real-world scenarios.\n",
    "\n",
    "#### Stochastic FrozenLake Environment\n",
    "\n",
    "In the stochastic FrozenLake environment (`is_slippery=True`):\n",
    "- The agent **intends** to move in a chosen direction\n",
    "- Due to the slippery ice, the agent may **slip** and move in unintended directions\n",
    "- Each action can result in **multiple possible outcomes** with different probabilities\n",
    "\n",
    "#### Mathematical Challenges\n",
    "\n",
    "Stochastic environments require us to:\n",
    "1. **Account for uncertainty** in transition probabilities during policy evaluation\n",
    "2. **Compute expected values** over all possible next states  \n",
    "3. **Handle multiple possible outcomes** for each action during policy improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acf3d6ce-56bc-4c01-914b-19a8fffe753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a26768a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dict = {0: \"Left\", 1: \"Down\", 2: \"Right\", 3: \"Up\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdcaf7c9-02a3-48fb-97c4-c74adc9a05ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy(policy, grid=(4, 4)):\n",
    "    \n",
    "    policy_map = np.empty(grid).astype(str)\n",
    "\n",
    "    for r in range(grid[0]):\n",
    "        for c in range(grid[1]):\n",
    "            index =  r * grid[0] +  c\n",
    "                \n",
    "            selected_action = action_dict[policy[index]]\n",
    "            selected_action = selected_action[0]\n",
    "\n",
    "            policy_map[r, c] = selected_action\n",
    "\n",
    "\n",
    "    print(\"Policy\")\n",
    "    print(policy_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a08dcd3-0633-4cb0-ab6b-39e4171a643c",
   "metadata": {},
   "source": [
    "#### Stochastic Policy Evaluation\n",
    "\n",
    "In stochastic environments, **policy evaluation** must account for the probabilistic nature of state transitions. Unlike deterministic environments where each action leads to exactly one next state, here we must compute **expected values** over all possible outcomes.\n",
    "\n",
    "##### Mathematical Formulation\n",
    "\n",
    "For a deterministic policy $\\pi(s)$ in a stochastic environment, the Bellman equation becomes:\n",
    "\n",
    "$$V^\\pi(s) = \\sum_{s'} P(s'|s,\\pi(s))[R(s,\\pi(s),s') + \\gamma V^\\pi(s')]$$\n",
    "\n",
    "Since multiple next states $s'$ are possible with different probabilities, we must:\n",
    "\n",
    "1. **Identify all possible transitions**: For action $a = \\pi(s)$, find all $(s', p, r)$ tuples\n",
    "2. **Compute weighted sum**: Sum over all possible outcomes weighted by their probabilities\n",
    "3. **Apply Bellman operator**: $V_{k+1}^\\pi(s) = \\sum_{s'} P(s'|s,\\pi(s))[R + \\gamma V_k^\\pi(s')]$\n",
    "\n",
    "##### Key Differences from Deterministic Case\n",
    "\n",
    "- **Multiple outcomes per action**: Each action can lead to several possible next states\n",
    "- **Probability weighting**: Must multiply each outcome by its transition probability\n",
    "- **Expected value computation**: The value function represents expected cumulative reward\n",
    "\n",
    "The implementation iterates over all possible transitions for the policy-selected action and computes the expected value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc77ea51-1d33-4fd5-b4b8-bae19320f99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, gamma=0.99, iterations=1000, epsilon=1e-10):\n",
    "\n",
    "    V = np.zeros(shape=env.observation_space.n)\n",
    "\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        V_k = np.copy(V)\n",
    "\n",
    "        for s in range(env.observation_space.n):\n",
    "\n",
    "            # Action given by policy, But can't take directly as of stochastic env\n",
    "            wanted_action = policy[s]\n",
    "            possible_actions = env.unwrapped.P[s][wanted_action]\n",
    "\n",
    "            V_s = 0\n",
    "\n",
    "            for prob, s_next, reward, terminal in possible_actions:\n",
    "                \n",
    "                V_s += prob * (reward + gamma * V_k[s_next])\n",
    "\n",
    "            V[s] = V_s\n",
    "\n",
    "        if np.max(np.abs(V - V_k)) < epsilon:\n",
    "            break\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24ad89d-8e6e-4023-b1e6-17a88df593bc",
   "metadata": {},
   "source": [
    "#### Stochastic Policy Improvement\n",
    "\n",
    "**Policy improvement** in stochastic environments requires computing **action-value functions** (Q-values) for all possible actions, accounting for the probabilistic transitions.\n",
    "\n",
    "##### Mathematical Foundation\n",
    "\n",
    "Given the value function $V^\\pi(s)$ from policy evaluation, we compute Q-values for each action:\n",
    "\n",
    "$$Q^\\pi(s,a) = \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V^\\pi(s')]$$\n",
    "\n",
    "The **improved policy** is then:\n",
    "$$\\pi'(s) = \\arg\\max_{a \\in A} Q^\\pi(s,a)$$\n",
    "\n",
    "##### Stochastic Q-Value Computation\n",
    "\n",
    "For each state $s$ and action $a$:\n",
    "\n",
    "1. **Enumerate all possible transitions**: Find all $(prob, s', reward, done)$ tuples for $(s,a)$\n",
    "2. **Compute expected Q-value**: $Q(s,a) = \\sum_{i} prob_i \\cdot (reward_i + \\gamma \\cdot V(s'_i))$\n",
    "3. **Select greedy action**: Choose action with maximum Q-value\n",
    "\n",
    "##### Handling Uncertainty\n",
    "\n",
    "Unlike deterministic environments, each action's value depends on:\n",
    "- **All possible next states** it can lead to\n",
    "- **Transition probabilities** to each next state  \n",
    "- **Rewards** associated with each transition\n",
    "\n",
    "The algorithm must evaluate the **expected utility** of each action considering all possible outcomes, making the policy robust to environmental uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b436a000-6718-4687-a847-f16b9ec7253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(values, gamma=0.99):\n",
    "    new_policy = np.zeros(env.observation_space.n)\n",
    "\n",
    "\n",
    "    for s in range(env.observation_space.n):\n",
    "\n",
    "        Q_s = []\n",
    "\n",
    "        for wanted_action in range(env.action_space.n):\n",
    "            possible_actions = env.unwrapped.P[s][wanted_action]\n",
    "\n",
    "            Q_sa = 0\n",
    "\n",
    "            for prob, s_next, reward, terminal in possible_actions:\n",
    "                Q_sa += prob * (reward + gamma * values[s_next])\n",
    "\n",
    "            Q_s.append(Q_sa)\n",
    "\n",
    "        best_action = np.argmax(Q_s)\n",
    "\n",
    "\n",
    "        new_policy[s] = best_action\n",
    "\n",
    "\n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f16359-f2fd-49d5-945f-a57283ccde6f",
   "metadata": {},
   "source": [
    "#### Complete Stochastic Policy Iteration\n",
    "\n",
    "The **complete policy iteration algorithm** for stochastic environments follows the same two-step process as deterministic environments, but with modified computations to handle probabilistic transitions.\n",
    "\n",
    "##### Algorithm Steps\n",
    "\n",
    "1. **Initialize**: Start with arbitrary policy $\\pi_0$\n",
    "2. **Repeat until convergence**:\n",
    "   - **Stochastic Policy Evaluation**: Compute $V^{\\pi_k}$ using expected values over all possible transitions\n",
    "   - **Stochastic Policy Improvement**: Update $\\pi_{k+1}$ by maximizing expected Q-values\n",
    "3. **Convergence**: Stop when $\\pi_{k+1} = \\pi_k$\n",
    "\n",
    "##### Key Adaptations for Stochastic Environments\n",
    "\n",
    "**Policy Evaluation Adaptation**:\n",
    "- Must iterate over **all possible outcomes** for each policy action\n",
    "- Compute **probability-weighted** value updates\n",
    "- Handle **multiple transition probabilities** per action\n",
    "\n",
    "**Policy Improvement Adaptation**:\n",
    "- Calculate **expected Q-values** for all actions considering uncertainty\n",
    "- Select action that maximizes **expected utility**\n",
    "- Account for **risk** inherent in stochastic transitions\n",
    "\n",
    "##### Convergence Properties\n",
    "\n",
    "- **Guaranteed convergence** to optimal policy $\\pi^*$ (same as deterministic case)\n",
    "- **Finite convergence** in finite state and action spaces\n",
    "- **Robustness** to environmental uncertainty through expected value computations\n",
    "\n",
    "The algorithm produces policies that are **optimal under uncertainty**, maximizing expected cumulative reward despite stochastic transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "027b2e33-fd5e-4a8b-a297-f460abfd9902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, num_iterations=1000, gamma=0.99):\n",
    "\n",
    "    policy = np.random.randint(low=0, high=env.action_space.n, size=(env.observation_space.n))\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "\n",
    "        values = policy_evaluation(policy, gamma=gamma, iterations=num_iterations)\n",
    "        update_policy = policy_improvement(values, gamma=gamma)\n",
    "\n",
    "        # Convergence\n",
    "        if np.all(policy == update_policy):\n",
    "            break\n",
    "\n",
    "        policy = update_policy\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d7caf4-2f01-4471-b7b3-43d9291d0710",
   "metadata": {},
   "source": [
    "#### Testing the Optimal Policy in Stochastic Environment\n",
    "\n",
    "Now we apply our policy iteration algorithm to the **stochastic FrozenLake** environment and evaluate its performance.\n",
    "\n",
    "##### Environment Setup\n",
    "- **Stochastic transitions**: `is_slippery=True` introduces randomness\n",
    "- **Uncertainty**: Agent may slip and move in unintended directions\n",
    "- **Challenge**: Policy must be robust to unpredictable outcomes\n",
    "\n",
    "##### Performance Evaluation\n",
    "Since the environment is stochastic, we cannot expect 100% success rate even with the optimal policy. Instead, we:\n",
    "\n",
    "1. **Run multiple episodes** to estimate average performance\n",
    "2. **Measure success rate** over many trials\n",
    "3. **Account for inherent randomness** in environment dynamics\n",
    "\n",
    "##### Expected Results\n",
    "The optimal policy should:\n",
    "- **Maximize expected reward** despite uncertainty\n",
    "- **Achieve reasonable success rate** given environmental constraints\n",
    "- **Balance risk and reward** in action selection\n",
    "\n",
    "Let's compute the optimal policy and evaluate its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a52bb277-af4f-468b-9eff-77c55ef33cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', map_name='4x4', render_mode='rgb_array', is_slippery=True)\n",
    "\n",
    "learned_policy = policy_iteration(env, gamma=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e902160",
   "metadata": {},
   "source": [
    "### Play games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beb8c27b-51f9-444f-871d-505ec94d887d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful game ratio: 0.819\n"
     ]
    }
   ],
   "source": [
    "num_games=1000\n",
    "max_steps=500\n",
    "\n",
    "game_success=0\n",
    "\n",
    "for _ in range(num_games):\n",
    "    observation, _ = env.reset()\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "\n",
    "        action = int(learned_policy[observation])\n",
    "\n",
    "        observation, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            if reward > 0:\n",
    "                game_success += 1\n",
    "            break\n",
    "\n",
    "successful_game_ratio = game_success / num_games\n",
    "print(f\"Successful game ratio: {successful_game_ratio}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1810f00b-28fe-4fb9-9307-ed82ba0521b0",
   "metadata": {},
   "source": [
    "### Monte Carlo Methods for Reinforcement Learning\n",
    "\n",
    "**Monte Carlo (MC)** methods are model-free approaches that learn optimal policies through experience sampling, without requiring knowledge of environment dynamics.\n",
    "\n",
    "**Key Principles:**\n",
    "- Learn from complete episodes (trajectories)\n",
    "- Estimate action-value function $Q(s,a)$ from sample returns\n",
    "\n",
    "- Use first-visit or every-visit averaging**Algorithm:** Sample episodes → Compute returns → Update Q-values → Improve policy\n",
    "\n",
    "- Apply $\\epsilon$-greedy exploration for policy improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ad58bb",
   "metadata": {},
   "source": [
    "#### Environment Setup\n",
    "\n",
    "We use the stochastic **FrozenLake** environment where transitions are probabilistic, making it ideal for demonstrating model-free learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9122fcb-04fc-4daa-ab01-a40ea8f04ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4679c2-4c15-4278-9bb4-2064d8530b3b",
   "metadata": {},
   "source": [
    "#### Episode Generation\n",
    "\n",
    "Generate episodes using **$\\epsilon$-greedy exploration**: with probability $\\epsilon$, select random action (explore); otherwise, follow policy (exploit). Each episode provides state-action-reward sequences for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21be599a-3d2f-4c11-afbe-796b6640e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_trajectory(pi, env, max_steps=50, epsilon=0.1):\n",
    "    done = False\n",
    "    trajectory = []\n",
    "    num_steps = 0\n",
    "\n",
    "    state, _ =  env.reset()\n",
    "\n",
    "    while not done:\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample() # Explore random action\n",
    "\n",
    "        else: \n",
    "            action = int(pi[state]) # exploit from best known\n",
    "\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        experience = (state, int(action), reward, next_state, done)\n",
    "        trajectory.append(experience)\n",
    "\n",
    "        num_steps += 1\n",
    "\n",
    "        if num_steps >= max_steps:\n",
    "            # No success\n",
    "            done = False\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374fe51e",
   "metadata": {},
   "source": [
    "#### Sample Episode\n",
    "\n",
    "Test trajectory generation with a random policy. Each trajectory contains $(s_t, a_t, r_t, s_{t+1}, done_t)$ tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a37883f2-eebf-4b04-aed5-26ae1d74e75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 3, 0.0, 0, False),\n",
       " (0, 3, 0.0, 0, False),\n",
       " (0, 3, 0.0, 0, False),\n",
       " (0, 3, 0.0, 0, False),\n",
       " (0, 3, 0.0, 0, False),\n",
       " (0, 3, 0.0, 0, False),\n",
       " (0, 3, 0.0, 0, False),\n",
       " (0, 3, 0.0, 1, False),\n",
       " (1, 1, 0.0, 5, True)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = np.random.randint(env.action_space.n, size=(env.observation_space.n,))\n",
    "trajectory = sample_trajectory(policy, env)\n",
    "\n",
    "trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126244c4",
   "metadata": {},
   "source": [
    "#### Example Returns\n",
    "\n",
    "Demonstrate return calculation for the sampled episode. Each $(s,a)$ pair gets its first-visit return value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f980df-9b41-4454-a69f-c38e8cb63795",
   "metadata": {},
   "source": [
    "#### Return Calculation\n",
    "\n",
    "Compute discounted returns $G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$ by working backwards from episode end. Uses **first-visit** Monte Carlo approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb6a5da3-ba3a-465b-ae7f-35f3e6eb174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(trajectory, gamma=0.99):\n",
    "    returns = {}\n",
    "    G = 0\n",
    "    for t in reversed(trajectory):\n",
    "        state, action, reward, _, _ = t\n",
    "\n",
    "        G = reward + gamma*G\n",
    "\n",
    "        # first visit\n",
    "        if (state, action) not in returns:\n",
    "            returns[(state, action)] = G\n",
    "\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "861cd437-0770-46a7-aa19-a7f993e809de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 1): 0.0, (0, 3): 0.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_returns(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3628792-1a1c-4672-a0ae-a6a237bfed19",
   "metadata": {},
   "source": [
    "#### Q-Value Estimation\n",
    "\n",
    "**Monte Carlo estimation**: $Q(s,a) = \\text{average of returns following first visits to } (s,a)$. Collect returns from multiple episodes and average them for each state-action pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a57c2acb-bea9-4b82-84b7-f65b10778daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_estimate(pi, env, gamma=0.99, max_steps=50, num_episode=5000):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    returns = {(s, a): [] for s in range(env.observation_space.n) for a in range(env.action_space.n)}\n",
    "\n",
    "    for _ in range(num_episode):\n",
    "        trajectory = sample_trajectory(pi, env, max_steps)\n",
    "\n",
    "        returns_for_trajectory = compute_returns(trajectory, gamma)\n",
    "\n",
    "        for (state, action), G in returns_for_trajectory.items():\n",
    "            returns[(state, action)].append(G)\n",
    "\n",
    "    for (state, action), returns_list in returns.items():\n",
    "\n",
    "        if len(returns_list) > 0:\n",
    "            Q[state, action] =  np.mean(returns_list)\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f763904-d0b1-47aa-acbb-073b063aacf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01529027, 0.0237408 , 0.02221273, 0.02070629],\n",
       "       [0.01283238, 0.02185069, 0.01815294, 0.02728737],\n",
       "       [0.04559264, 0.01444578, 0.06760458, 0.0209092 ],\n",
       "       [0.        , 0.        , 0.00674461, 0.068614  ],\n",
       "       [0.01746869, 0.        , 0.04029022, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.0743805 , 0.02390488, 0.074077  , 0.03923626],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.01274572, 0.04669826, 0.        ],\n",
       "       [0.0106682 , 0.        , 0.        , 0.        ],\n",
       "       [0.22777507, 0.17779144, 0.17241118, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.01272857, 0.107811  , 0.2277    , 0.21673289],\n",
       "       [0.09      , 0.4841629 , 0.98759975, 0.5       ],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monte_carlo_estimate(policy, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51201e6-8711-498f-a0a2-3f6fdd9aefcb",
   "metadata": {},
   "source": [
    "#### Policy Improvement\n",
    "\n",
    "**Greedy policy improvement**: $\\pi'(s) = \\arg\\max_a Q(s,a)$. Select action with highest estimated Q-value for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d3f916d-7d2a-4977-98be-884e12a77b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(Q):\n",
    "    return np.argmax(Q, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85ad0d8",
   "metadata": {},
   "source": [
    "#### Complete Monte Carlo Algorithm\n",
    "\n",
    "**Monte Carlo Policy Iteration**: Alternate between policy evaluation (estimate Q-values) and policy improvement (greedy update) until convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf035a2b-152c-475f-9662-7ac4e62c2ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_policy_iteration(env, gamma=0.99, max_steps=50, num_episodes=10000):\n",
    "    policy = np.random.randint(env.action_space.n, size=(env.observation_space.n,))\n",
    "\n",
    "    while True:\n",
    "        Q = monte_carlo_estimate(policy, env, gamma, max_steps, num_episodes)\n",
    "\n",
    "        new_policy = policy_improvement(Q)\n",
    "\n",
    "        if np.array_equal(policy, new_policy):\n",
    "            break\n",
    "\n",
    "        policy = new_policy\n",
    "\n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadb6ffd",
   "metadata": {},
   "source": [
    "#### Learn Optimal Policy\n",
    "\n",
    "Run complete Monte Carlo policy iteration to find the optimal policy for the stochastic FrozenLake environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c56e6e53-8b41-4a5d-b0f9-e5fc5bca0ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 0 3 0 0 2 0 3 1 0 0 0 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "optimal_policy, optimal_Q = monte_carlo_policy_iteration(env)\n",
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a910ec-e501-40a7-b0de-48ce02378316",
   "metadata": {},
   "source": [
    "#### Policy Evaluation\n",
    "\n",
    "Test the learned optimal policy by measuring success rate over multiple episodes. This validates the effectiveness of Monte Carlo learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09a6ad5a-d7ed-4b4b-bacd-57f79f937291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Success Rate: 54.60%\n"
     ]
    }
   ],
   "source": [
    "def test_policy(policy, env, num_episodes=500):\n",
    "    success_count = 0\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = policy[state]\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            if done and reward == 1.0:  # Reached the goal\n",
    "                success_count += 1\n",
    "\n",
    "    success_rate = success_count / num_episodes\n",
    "    print(f\"Policy Success Rate: {success_rate * 100:.2f}%\")\n",
    "\n",
    "test_policy(optimal_policy, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736c9ba8-0fb6-49c7-ab24-dec525cbb983",
   "metadata": {},
   "source": [
    "### Online Monte Carlo\n",
    "\n",
    "**Online Monte Carlo** updates Q-values incrementally during learning rather than storing all returns and computing averages at the end.\n",
    "\n",
    "**Key Advantage:** Memory efficient - no need to store all episode returns\n",
    "\n",
    "**Update Rule:** \n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\frac{1}{N(s,a)} [G - Q(s,a)]$$\n",
    "\n",
    "where $N(s,a)$ is the visit count for state-action pair $(s,a)$.\n",
    "\n",
    "Applied to Q-learning: each new return $G$ updates the current Q-value estimate using the difference between the new return and current estimate, scaled by the inverse visit count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fc70657-6cf3-4088-af10-ce154ed83ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_monte_carlo_estimate(pi, env, gamma=0.99, max_steps=50, num_episodes=10000):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    N = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        trajectory = sample_trajectory(pi, env, max_steps)\n",
    "\n",
    "        returns = compute_returns(trajectory, gamma)\n",
    "\n",
    "        for (state, action), G in returns.items():\n",
    "            Q[state, action] = Q[state, action] + (G - Q[state, action]) / (N[state, action] + 1)\n",
    "            N[state, action] += 1\n",
    "    \n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ada1622-0b41-454b-999e-5a1d25a3eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_monte_carlo_policy_iteration(env, gamma=0.99, max_steps=50, num_episodes=10000):\n",
    "    policy = np.random.randint(env.action_space.n, size=(env.observation_space.n,))\n",
    "\n",
    "    while True:\n",
    "        Q = online_monte_carlo_estimate(policy, env, gamma, max_steps, num_episodes)\n",
    "\n",
    "        new_policy = policy_improvement(Q)\n",
    "\n",
    "        if np.array_equal(policy, new_policy):\n",
    "            break\n",
    "\n",
    "        policy = new_policy\n",
    "\n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2adf6506-fa6e-48e9-ac69-4dd6bcb89548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 0 0 0 0 2 0 3 1 0 0 0 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "optimal_policy, optimal_Q = monte_carlo_policy_iteration(env)\n",
    "\n",
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afcd1abe-ab2a-48c0-aaab-8d9d141e6a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Success Rate: 77.60%\n"
     ]
    }
   ],
   "source": [
    "test_policy(optimal_policy, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed6d539-1dca-401c-8181-96a82c2f82b4",
   "metadata": {},
   "source": [
    "### Learning Rate\n",
    "\n",
    "**Learning Rate Decay** replaces the visit count $N(s,a)$ with a time-decaying learning rate $\\alpha_t$ that decreases over episodes.\n",
    "\n",
    "**Key Benefits:**\n",
    "- More controlled learning schedule compared to $1/N(s,a)$ decay\n",
    "- Prevents learning rate from becoming too small too quickly\n",
    "- Better convergence properties in practice\n",
    "\n",
    "**Update Rule:** \n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha_t [G - Q(s,a)]$$\n",
    "\n",
    "where $\\alpha_t = \\max(\\alpha_{min}, \\alpha_{start} \\cdot \\gamma^t)$ provides exponential decay with a minimum threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23f73d23-0bfe-43b9-8807-b7818b06cb1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def lr_scheduler(start_val, min_val, decay_factor, num_episodes):\n",
    "\n",
    "    alphas=[start_val*decay_factor**ep for ep in range(num_episodes)]\n",
    "    alphas=[a if a>=min_val else min_val for a in alphas]\n",
    "\n",
    "    return alphas\n",
    "\n",
    "alphas = lr_scheduler(1.0, 0.01, 0.99, 5000)\n",
    "\n",
    "plt.plot(alphas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12bba62c-dba3-40ed-a9d4-08f635982c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_monte_carlo_estimation_w_lr(pi,\n",
    "                                       env,\n",
    "                                       gamma=0.99,\n",
    "                                       max_steps=50,\n",
    "                                       num_episodes=5000,\n",
    "                                       lr_start_val=0.8, \n",
    "                                       lr_min_val=0.01, \n",
    "                                       lr_decay_factor=0.99):\n",
    "\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    alphas = lr_scheduler(lr_start_val, lr_min_val, lr_decay_factor, num_episodes)\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        trajectory = sample_trajectory(pi, env, max_steps)\n",
    "\n",
    "        returns = compute_returns(trajectory, gamma)\n",
    "\n",
    "        for (state, action), G in returns.items():\n",
    "        \n",
    "            Q[state, action] = Q[state, action] + alphas[i] * (G - Q[state, action])\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6b04278-ceff-4e01-aca1-eb46d6628658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 1 3 0 0 2 0 3 1 1 0 0 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "def monte_carlo_policy_iteration(env, \n",
    "                                 gamma=0.99, \n",
    "                                 max_steps=50, \n",
    "                                 num_episodes=10000,\n",
    "                                 lr_start_val=0.6, \n",
    "                                 lr_min_val=0.01, \n",
    "                                 lr_decay_factor=0.98):\n",
    "    policy = np.random.choice(env.action_space.n, size=(env.observation_space.n, ))\n",
    "\n",
    "    while True:\n",
    "        Q = online_monte_carlo_estimation_w_lr(policy, env, gamma, max_steps, num_episodes, lr_start_val, lr_min_val, lr_decay_factor)\n",
    "        new_policy = policy_improvement(Q)\n",
    "\n",
    "        if np.array_equal(policy, new_policy):\n",
    "            break\n",
    "\n",
    "        policy = new_policy\n",
    "\n",
    "    return policy, Q\n",
    "\n",
    "optimal_policy, optimal_Q = monte_carlo_policy_iteration(env)\n",
    "\n",
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca206f39-b33d-4131-add7-35e4cc474020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Success Rate: 40.40%\n"
     ]
    }
   ],
   "source": [
    "test_policy(optimal_policy, env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

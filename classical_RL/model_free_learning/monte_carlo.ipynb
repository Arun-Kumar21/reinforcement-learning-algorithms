{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1810f00b-28fe-4fb9-9307-ed82ba0521b0",
   "metadata": {},
   "source": [
    "### Monte Carlo Methods for Reinforcement Learning\n",
    "\n",
    "**Monte Carlo (MC)** methods are model-free approaches that learn optimal policies through experience sampling, without requiring knowledge of environment dynamics.\n",
    "\n",
    "**Key Principles:**\n",
    "- Learn from complete episodes (trajectories)\n",
    "- Estimate action-value function $Q(s,a)$ from sample returns\n",
    "\n",
    "- Use first-visit or every-visit averaging**Algorithm:** Sample episodes → Compute returns → Update Q-values → Improve policy\n",
    "\n",
    "- Apply $\\epsilon$-greedy exploration for policy improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ad58bb",
   "metadata": {},
   "source": [
    "#### Environment Setup\n",
    "\n",
    "We use the stochastic **FrozenLake** environment where transitions are probabilistic, making it ideal for demonstrating model-free learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9122fcb-04fc-4daa-ab01-a40ea8f04ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4679c2-4c15-4278-9bb4-2064d8530b3b",
   "metadata": {},
   "source": [
    "#### Episode Generation\n",
    "\n",
    "Generate episodes using **$\\epsilon$-greedy exploration**: with probability $\\epsilon$, select random action (explore); otherwise, follow policy (exploit). Each episode provides state-action-reward sequences for learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374fe51e",
   "metadata": {},
   "source": [
    "#### Sample Episode\n",
    "\n",
    "Test trajectory generation with a random policy. Each trajectory contains $(s_t, a_t, r_t, s_{t+1}, done_t)$ tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21be599a-3d2f-4c11-afbe-796b6640e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_trajectory(pi, env, max_steps=50, epsilon=0.1):\n",
    "    done = False\n",
    "    trajectory = []\n",
    "    num_steps = 0\n",
    "\n",
    "    state, _ =  env.reset()\n",
    "\n",
    "    while not done:\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample() # Explore random action\n",
    "\n",
    "        else: \n",
    "            action = int(pi[state]) # exploit from best known\n",
    "\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        experience = (state, int(action), reward, next_state, done)\n",
    "        trajectory.append(experience)\n",
    "\n",
    "        num_steps += 1\n",
    "\n",
    "        if num_steps >= max_steps:\n",
    "            # No success\n",
    "            done = False\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a37883f2-eebf-4b04-aed5-26ae1d74e75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1, 0.0, 1, False),\n",
       " (1, 3, 0.0, 2, False),\n",
       " (2, 1, 0.0, 3, False),\n",
       " (3, 3, 0.0, 3, False),\n",
       " (3, 0, 0.0, 7, True)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = np.random.randint(env.action_space.n, size=(env.observation_space.n,))\n",
    "trajectory = sample_trajectory(policy, env)\n",
    "\n",
    "trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126244c4",
   "metadata": {},
   "source": [
    "#### Example Returns\n",
    "\n",
    "Demonstrate return calculation for the sampled episode. Each $(s,a)$ pair gets its first-visit return value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f980df-9b41-4454-a69f-c38e8cb63795",
   "metadata": {},
   "source": [
    "#### Return Calculation\n",
    "\n",
    "Compute discounted returns $G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$ by working backwards from episode end. Uses **first-visit** Monte Carlo approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb6a5da3-ba3a-465b-ae7f-35f3e6eb174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(trajectory, gamma=0.99):\n",
    "    returns = {}\n",
    "    G = 0\n",
    "    for t in reversed(trajectory):\n",
    "        state, action, reward, _, _ = t\n",
    "\n",
    "        G = reward + gamma*G\n",
    "\n",
    "        # first visit\n",
    "        if (state, action) not in returns:\n",
    "            returns[(state, action)] = G\n",
    "\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639ff6ab",
   "metadata": {},
   "source": [
    "#### Estimated Q-Values\n",
    "\n",
    "Compute Q-values for the random policy using Monte Carlo sampling over multiple episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "861cd437-0770-46a7-aa19-a7f993e809de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(3, 0): 0.0, (3, 3): 0.0, (2, 1): 0.0, (1, 3): 0.0, (0, 1): 0.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_returns(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3628792-1a1c-4672-a0ae-a6a237bfed19",
   "metadata": {},
   "source": [
    "#### Q-Value Estimation\n",
    "\n",
    "**Monte Carlo estimation**: $Q(s,a) = \\text{average of returns following first visits to } (s,a)$. Collect returns from multiple episodes and average them for each state-action pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85ad0d8",
   "metadata": {},
   "source": [
    "#### Complete Monte Carlo Algorithm\n",
    "\n",
    "**Monte Carlo Policy Iteration**: Alternate between policy evaluation (estimate Q-values) and policy improvement (greedy update) until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadb6ffd",
   "metadata": {},
   "source": [
    "#### Learn Optimal Policy\n",
    "\n",
    "Run complete Monte Carlo policy iteration to find the optimal policy for the stochastic FrozenLake environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a57c2acb-bea9-4b82-84b7-f65b10778daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_estimate(pi, env, gamma=0.99, max_steps=50, num_episode=5000):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    returns = {(s, a): [] for s in range(env.observation_space.n) for a in range(env.action_space.n)}\n",
    "\n",
    "    for _ in range(num_episode):\n",
    "        trajectory = sample_trajectory(pi, env, max_steps)\n",
    "\n",
    "        returns_for_trajectory = compute_returns(trajectory, gamma)\n",
    "\n",
    "        for (state, action), G in returns_for_trajectory.items():\n",
    "            returns[(state, action)].append(G)\n",
    "\n",
    "    for (state, action), returns_list in returns.items():\n",
    "\n",
    "        if len(returns_list) > 0:\n",
    "            Q[state, action] =  np.mean(returns_list)\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f763904-d0b1-47aa-acbb-073b063aacf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00332493, 0.00992774, 0.00643614, 0.02329738],\n",
       "       [0.01791337, 0.02646487, 0.01079234, 0.01779288],\n",
       "       [0.03384885, 0.02789411, 0.04023248, 0.03281185],\n",
       "       [0.02208107, 0.02071262, 0.02101518, 0.03204948],\n",
       "       [0.0059296 , 0.00190535, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.11359636, 0.03988523, 0.0306771 , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.00167689, 0.01033685, 0.03358163, 0.02283056],\n",
       "       [0.        , 0.        , 0.        , 0.07182285],\n",
       "       [0.21436083, 0.0825    , 0.1321796 , 0.07924917],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.740025  , 0.        , 0.08839286],\n",
       "       [0.33      , 0.398     , 0.56437086, 0.39082782],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monte_carlo_estimate(policy, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51201e6-8711-498f-a0a2-3f6fdd9aefcb",
   "metadata": {},
   "source": [
    "#### Policy Improvement\n",
    "\n",
    "**Greedy policy improvement**: $\\pi'(s) = \\arg\\max_a Q(s,a)$. Select action with highest estimated Q-value for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d3f916d-7d2a-4977-98be-884e12a77b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(Q):\n",
    "    return np.argmax(Q, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf035a2b-152c-475f-9662-7ac4e62c2ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_policy_iteration(env, gamma=0.99, max_steps=50, num_episodes=10000):\n",
    "    policy = np.random.randint(env.action_space.n, size=(env.observation_space.n,))\n",
    "\n",
    "    while True:\n",
    "        Q = monte_carlo_estimate(policy, env, gamma, max_steps, num_episodes)\n",
    "\n",
    "        new_policy = policy_improvement(Q)\n",
    "\n",
    "        if np.array_equal(policy, new_policy):\n",
    "            break\n",
    "\n",
    "        policy = new_policy\n",
    "\n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c56e6e53-8b41-4a5d-b0f9-e5fc5bca0ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 3 3 0 0 0 0 3 1 0 0 0 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "optimal_policy, optimal_Q = monte_carlo_policy_iteration(env)\n",
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a910ec-e501-40a7-b0de-48ce02378316",
   "metadata": {},
   "source": [
    "#### Policy Evaluation\n",
    "\n",
    "Test the learned optimal policy by measuring success rate over multiple episodes. This validates the effectiveness of Monte Carlo learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09a6ad5a-d7ed-4b4b-bacd-57f79f937291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Success Rate: 81.80%\n"
     ]
    }
   ],
   "source": [
    "def test_policy(policy, env, num_episodes=500):\n",
    "    success_count = 0\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = policy[state]\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            if done and reward == 1.0:  # Reached the goal\n",
    "                success_count += 1\n",
    "\n",
    "    success_rate = success_count / num_episodes\n",
    "    print(f\"Policy Success Rate: {success_rate * 100:.2f}%\")\n",
    "\n",
    "test_policy(optimal_policy, env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
